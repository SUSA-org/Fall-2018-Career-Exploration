{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6: Linear Regression, Validation, and Fitting\n",
    "## 10/16/18\n",
    "\n",
    "### Hosted by and maintained by the [Statistics Undergraduate Students Association (SUSA)](https://susa.berkeley.edu). Originally authored by [Calvin Chen](mailto:chencalvin99@berkeley.edu) and [Rosa Choe](mailto:rosachoe@berkeley.edu).\n",
    "\n",
    "\n",
    "### Table Of Contents\n",
    "* [Introduction](#intro)\n",
    "    * [What Is A Model?](#what_model)\n",
    "    * [Why Make A Model?](#why_model)\n",
    "    * [Categorical Variables](#categorical)\n",
    "* [Linear Regression](#linear_regression)\n",
    "    * [Simple Linear Regression](#simple)\n",
    "    * [Loss and the Line of Best Fit](#loss)\n",
    "    * [Ordinary Least Squares](#ols)\n",
    "    * [Making the Model](#making_model)\n",
    "    * [Interpreting the Model](#interpreting_model)\n",
    "    * [Assessing the Model](#assessment)\n",
    "        * [Coefficient of Determination ($R^2$)](#r_squared)\n",
    "        * [Residual Plots](#residual_plots)\n",
    "    * [When Can I Use A Linear Model?](#when_to_use)\n",
    "    * [Multiple Linear Regression](#multiple)\n",
    "* [Fitting](#fitting)\n",
    "    * [Underfitting](#underfitting)\n",
    "        * [Data Transformation](#data_transformation)\n",
    "        * [Polynomial Regression](#polynomial)\n",
    "    * [Overfitting](#overfitting)\n",
    "        * [$R^2$ and Overfitting](#r_squared_overfitting)\n",
    "        * [Overfitting Demo](#overfitting_demo)\n",
    "* [Validation](#validation)\n",
    "    * [Training Error and Test Error](#error)\n",
    "* [Recap](#recap)\n",
    "* [Exercises](#exercises)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from plotting import overfittingDemo, plot_multiple_linear_regression\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "Imagine you've just met an alien who is on a mission to catalog and describe all life on Earth. They've asked you to describe what a \"horse\" is. How would you describe this to them?\n",
    "\n",
    "<a id='what_model'></a>\n",
    "### What Is A Model?\n",
    "A model is a simplification of reality. You want it to be general enough that it can accurately describe more than just a handful of examples of what it's supposed to represent.\n",
    "\n",
    "<table bgcolor=white><tr>\n",
    "    <td><img src='model_reality.png' width=400 /></td>\n",
    "    <td width=100></td>\n",
    "    <td><img src='model_reality_2.png' width=400 /></td>\n",
    "</tr></table>\n",
    "\n",
    "Besides describing classes of things, like *horse*, *orange tabby cat*, or *genders*, models can also describe the relationship between things. Some of these might be familiar to you:\n",
    "<table>\n",
    "    <tr><td>Newton's Second Law</td><td>$F = ma$</td></tr>\n",
    "    <tr><td>Hooke's Law</td><td>$F = -kx$</td></tr>\n",
    "    <tr><td>Position of a falling ball</td><td>$y(t) = \\frac{1}{2}at^2$</td></tr>\n",
    "</table>\n",
    "\n",
    "<a id='why_model'></a>\n",
    "### Why Make A Model?\n",
    "The examples above may give you an idea of what kinds of models you'd want to make. You could make a model to describe something, whether that be a class of objects, like cats, or the relationship between multiple things, like mass, acceleration and force in Newton's Second Law. Once you have a model, you might want to use it to make predictions. As an example, maybe you'd like to be able to make a good guess for someone's weight based on their height â€“ you could make a model that describes the relationship between weight and height and use that model to predict weights.\n",
    "\n",
    "There's one thing you should always keep in mind! Just because you can make a model describing the relationship between two variables, and even if you can use this model to predict the value of one variable based on the value of the other, it doesn't mean that one causes the other. You may have heard this before as the difference between **correlation** and **causation**. A classic example is the relationship between ice cream sales and murder rates. Turns out, when ice cream sales rise, so do murder rates. Does this mean ice cream *causes* people to commit murder? Or get murdered? Nope!\n",
    "\n",
    "Today, we're going to learn how to make a linear model to describe the relationship between variables.\n",
    "\n",
    "<a id='categorical'></a>\n",
    "### Categorical Variables\n",
    "In order to make any model, we first need data. Some types of data are more appropriate for linear models, so let's talk about different types of variables. \n",
    "\n",
    "- **categorical/qualitative**: a variable that has discrete values that represent *categories*\n",
    "    - **ordinal**: a categorical variable whose categories have a clear *ordering*, so the categories have numerical meaning\n",
    "        - e.g. `first class`, `second class`\n",
    "    - **nominal**: a categorial variable whose categories exist by *name* only, with no inherent numerical value or ordering\n",
    "        - e.g. `female`, `male` \n",
    "- **quantitative**: a variable that's measured on a numeric scale\n",
    "    - **continuous**: a quantitative variable that can take on an infinite number of values\n",
    "        - e.g. `weight`, `temperature`\n",
    "    - **discrete**: a quantitative variable that can only take on certain values\n",
    "        - e.g. `birth year`, `temperature to the nearest degree`\n",
    "\n",
    "We'll come back to these terms later.\n",
    "\n",
    "<a id='linear_regression'></a>\n",
    "## Linear Regression\n",
    "**Linear regression** is a method of making linear models. Linear models is one kind of model, in which the relationship between the explanatory variables and the response variable can be described by a linear function. For now, you can just think of a linear function as a straight line, which takes us to *simple linear regression*.\n",
    "\n",
    "<a id='simple'></a>\n",
    "### Simple Linear Regression\n",
    "**Simple linear regression** is a special case of linear regression in which you only have one explanatory variable. As the name suggests, it models the relationship as a *line*. You may be familiar with the slope-intercept form of a line, and that's exactly how the linear model looks!\n",
    "\n",
    "$$y = mx+b$$\n",
    "\n",
    "Here, $y$ is the **response** or **dependent** variable we're trying to predict, and $x$ is an **explanatory** or **independent** variable used to predict $y$. In the case of our weight and height example, $y$ would represent weight, while $x$ represents height.\n",
    "\n",
    "Using known $x$'s, we want to accurately predict $y$ using the right $m$ and $b$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loss'></a>\n",
    "### Loss and the Line of Best Fit\n",
    "How do we figure out what $m$ and $b$ are? We do this by defining a *loss function*. The **loss function** is a function that measures how far off our model's estimated values are from the true values. We want our model to be as accurate as possible, so that means we want to minimize the error our model makes in predicting values. In other words, we want to minimize the loss. Another name for the line that minimizes the error is the **line of best fit**. It's a pretty descriptive name, since it's the line that fits our data the best. The *loss function* helps us define what is *best fit*. \n",
    "\n",
    "<img src='simple_linear.png' width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ols'></a>\n",
    "### Ordinary Least Squares\n",
    "In the case of linear regression, we use the method of **ordinary least squares (OLS)**, which minimizes the sum of squared residuals. A **residual** is the difference between the predicted value and the observed value for a given $x$. For a given observation $(x_i, y_i)$, the residual $e_i$ is calculated as:\n",
    "\n",
    "$$ \\underbrace{e_{i}}_{error} = \\underbrace{y_i}_{actual} - \\underbrace{\\hat{y_i}}_{predicted} = y_i - mx_i - b$$\n",
    "\n",
    "**Question**: Can you think of why we would want to *square* the residuals and sum them instead of just minimizing their sum?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to minimize the **residual sum of squares (RSS)**, what we're actually going to minimize is this:\n",
    "\n",
    "$$\\textit{RSS} = \\sum_{i=0}^n {e_i}^2 = \\sum_{i=0}^n (y_i - mx_i - b)^2$$\n",
    "\n",
    "By minimizing this function, we can solve for slope $m$ and the intercept $b$. The actual calculations for deriving the formulas that define these coefficients requires a bit of calculus, so we'll skip that part for now, but if you want to look into it more on your own you can check out [this link](http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf)! For now, we'll just tell you that $m$ and $b$ can be solved as:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\hat{b}&=\\bar {y}-\\hat{m}\\,{\\bar{x}},\\\\\n",
    "\\hat{m}&=\\frac{\\sum _{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar {y})}{\\sum _{i=1}^{n}(x_{i}-\\bar{x})^2}\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "This is pretty complicated! Luckily, you don't need to know any of this to make a linear model, but this is here for reference if you're interested in the math behind what we'll be getting into today. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='making_model'></a>\n",
    "### Making the Model\n",
    "In linear regression, the response variable should be continuous. The explanatory variables *can* be discrete and even categorical, and in a future lecture you'll learn how to use categorical variables in your models, but in simple linear regression they need to be continuous. For today we'll just be working with continuous variables! \n",
    "\n",
    "Let's revisit the `titanic` dataset you're familiar with and decide whether it's appropriate for making a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the titanic data from excel sheet\n",
    "titanic_df = pd.read_excel('titanic3.xls', 'titanic3', index_col=None, na_values=['NA'])\n",
    "\n",
    "# plotting ticket class vs. fare\n",
    "titanic_df.plot.scatter(\"pclass\", \"fare\")\n",
    "plt.title(\"Titanic: Ticket Class vs. Fare\")\n",
    "plt.xlabel(\"Ticket Class\")\n",
    "plt.ylabel(\"Fare\")\n",
    "\n",
    "# plotting age vs. fare\n",
    "titanic_df.plot.scatter(\"age\", \"fare\")\n",
    "plt.title(\"Titanic: Age vs. Fare\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Fare\")\n",
    "\n",
    "# plotting number of siblings vs. fare\n",
    "titanic_df.plot.scatter(\"sibsp\", \"fare\")\n",
    "plt.title(\"Titanic: # of Siblings/Spouses vs. Fare\")\n",
    "plt.xlabel(\"# of Siblings/Spouses\")\n",
    "plt.ylabel(\"Fare\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How would you describe these variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to find the new dataset we found to work with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = pd.read_csv(\"./mpg.csv\", index_col=\"name\") # load mpg dataset\n",
    "mpg = mpg.loc[mpg[\"horsepower\"] != '?'].astype(int) # remove columns with missing horsepower values\n",
    "mpg_train, mpg_test = train_test_split(mpg, test_size = .2, random_state = 0) # split into training set and test set\n",
    "mpg_train, mpg_validation = train_test_split(mpg_train, test_size = .5, random_state = 0)\n",
    "mpg_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've chosen the `mpg` dataset, which tells us various attributes of different cars, including a car's make and model, miles per gallon, number of cylinders, weight, and more! We're going to be trying to see which features affect a car's `mpg`, and our goal is to create a model that accurately predicts `mpg` given other attributes of the car. \n",
    "\n",
    "You'll notice that we separated the `mpg` data into two separate dataframes, `mpg_train` and `mpg_test`. We'll get into why in a later part of today's lecture, but for now, make sure to do all of your analysis and model creation on the `mpg_train` dataset! \n",
    "\n",
    "Try making some scatter plots of different variables as your `x` and `mpg` as your `y` using the `mpg_train` dataset below!\n",
    "\n",
    "*Hint:* Hitting `shift-tab` with the cursor on the name of a function will bring up helpful documentation about how to use the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_train.plot.scatter(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn`'s `linear_model` module makes it really easy to make linear models! There's a lot of different types of linear models implemented in the `linear_model` module, which you can take a look at [here](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) if you're interested, but for today we'll be using `LinearRegression`, which we've imported for you in the cell below. Try reading the documentation to figure out what the `fit()` function expects as input to correctly fit our model to the `mpg_train` data!\n",
    "\n",
    "*Hint:* if you want to select a subset of columns from a dataframe, pass in a list of column names, like `df[['col1', 'col2]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our linear regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "X = ...\n",
    "Y = ...\n",
    "\n",
    "# Fit the model to the data\n",
    "linear_model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've got it working you'll notice that it seems like nothing happened. However, behind the scenes, our `linear_model` variable has now been fit to the data we passed into the `fit()` function! We can see what the `slope` and `intercept` are by looking into the `coef_` and `intercept_` attributes of our `linear_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.coef_, linear_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that, while the `intercept_` is a single scalar value, `coef_` returns an array. This is because you can choose to fit your model to multiple explanatory variables (hence the list form of `feature_cols`). When you define multiple explanatory variables, the `coef_` will contain a separate coefficient for each explanatory variable you chose! You'll be able to explore that in a bit, but for now let's take a look at what our linear model looks like relative to our original data.\n",
    "\n",
    "We've provided the skeleton for a helper function called `overlay_simple_linear_model`. Try to fill out the function so that it plots a scatterplot with the linear model overlaid on top.\n",
    "\n",
    "*Hint:* If you press `tab` after a `[object].` or `[package].`, Jupyter will show you a list of valid functions defined for that object type or package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_simple_linear_model(data, x_name, y_name, linear_model):\n",
    "    \"\"\"\n",
    "    This function plots a simple linear model on top of the scatterplot of the data it was fit to.\n",
    "    \n",
    "    data(DataFrame): e.g. mpg_train\n",
    "    x_name(string): the name of the column representing the predictor variable\n",
    "    y_name(string): the name of the column representing the dependent/response variable\n",
    "    linear_model\n",
    "    \n",
    "    returns None but outputs linear model overlaid on scatterplot\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.arange(max(data[x_name])).reshape((-1, 1)) # an array of integers between 0 and the maximum value of the x_name column\n",
    "    y = linear_model.____ # replace ___ with correct function \n",
    "    \n",
    "    \n",
    "    data.plot.scatter(...) # scatter plot of x_name vs. y_name\n",
    "    \n",
    "    plt.plot(x, y, color='red')\n",
    "    plt.title(\"Linear Model vs. Data: \" + x_name + \" vs. \" + y_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wrote the function above correctly, this should produce a scatterplot with a line through it\n",
    "overlay_simple_linear_model(mpg_train, ..., \"mpg\", linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='interpreting_model'></a>\n",
    "### Interpreting the Model\n",
    "\n",
    "You're probably thinking \"COOL! This looks like a pretty good representation of the data! But what do these coefficients even mean?\" That is a great question! As you might have guessed, the `intercept` term is where our line intersects with the y-axis, or when our predictor variable has a value of 0. In relation to our model, it's our prediction for `mpg` given a predictor variable value of 0. The `slope` term is a little more complicated. Yes, it is the slope of the line, but how do we interpret it in the relationship between `mpg` and our explanator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assessment'></a>\n",
    "### Assessing the Model\n",
    "<a id='r_squared'></a>\n",
    "#### Coefficient of Determination ($R^2$)\n",
    "Another question you might have is, how do we know how good our model is? One way of measuring how well your model fits the data is the $R^2$ coefficient, or the **coefficient of determination**. Basically, what the $R^2$ represents is the proportion of variation in the response variable that is explained by the explanatory variables. If you want to look into the mathematical definition of $R^2$, you can check out the [Wikipedia page](https://en.wikipedia.org/wiki/Coefficient_of_determination)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain our model's $R^2$ value by using our `linear_model`'s `score()` function, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.score(...) # you'll only need to use variables that we've already defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! If you used `displacement`, our model accurately predicts 66% of the variation in `mpg`. Is this good? Since $R^2$ is a proportion, it's value is always between $0$ and $1$. An $R^2$ value of $0$ would mean that none of the variation of $y$ is explained by $x$, and our model does not explain anything about the data! Our model is essentially guessing the same value each time. An $R^2$ value of $1$ means that our model perfectly fits our data! So, the closer our $R^2$ value is to $1$, the better it fits our data.\n",
    "\n",
    "In the cell below, try making different simple linear models using different functions and seeing their corresponding $R^2$. Make sure to stick with one explanatory variable for now!\n",
    "\n",
    "**Exercise:** Can you think of a possible feature you could use to make our model have an $R^2$ value of $1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model2 = LinearRegression()\n",
    "\n",
    "X2 = ...\n",
    "\n",
    "... # fit linear_model2\n",
    "\n",
    "r_squared = ...\n",
    "\n",
    "r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='residual_plots'></a>\n",
    "#### Residual Plots\n",
    "Another way of analyzing your model is through *residual plots*. A **residual plot** is kind of what you'd think â€“ it plots your residuals against the corresponding $x$ values. If you see interesting patterns in your residual plot, it's indicative of some *bias* in your model â€“ your error isn't due to randomness in the data but because of an underlying problem in the way you've defined the relationship between your variables. \n",
    "\n",
    "Fill in the blanks in the `plot_simple_residuals()` function, so we can take a look at the residual plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_residuals(data, x_name, y_name, linear_model):\n",
    "    \"\"\"\n",
    "    This function plots a residual plot based off of a simple linear model \n",
    "    on top of the scatterplot of the data it was fit to.\n",
    "    \n",
    "    data(DataFrame): e.g. mpg_train\n",
    "    x_name(string): the name of the column representing the predictor variable\n",
    "    y_name(string): the name of the column representing the dependent/response variable\n",
    "    linear_model\n",
    "    \n",
    "    returns None but outputs residual plot resulting from linear model overlaid on scatterplot\n",
    "    \"\"\"\n",
    "    X = ...\n",
    "    Y = ...\n",
    "    residuals = ...\n",
    "    \n",
    "    plt.scatter(X, residuals)\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.title(\"Residual Plot: \" + x_name + \" vs. \" + y_name)\n",
    "    plt.xlabel(x_name)\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_residuals(mpg_train, ..., 'mpg', linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the residuals aren't scattered randomly around the x-axis. The points are more spread out vertically for smaller values of `displacement` and less scattered vertically for larger values. Furthermore, in the middle the residuals are mostly above the line, while on the left and right side, the residuals tend to be below the line. Such a pattern as this one suggests that our model isn't that great at describing the relationship between `displacement` and `mpg`, and there's some fundamental issue with the assumption that the relationship can be modeled by a simple linear relationship. [Here](http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/)'s some more information about how to interpret different patterns in residual plots and how you can change your model to fix these errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='when_to_use'></a>\n",
    "### When Can I Use a Linear Model?\n",
    "Let's talk about some of the assumptions of linear regression, so you know when it's appropriate to use one. \n",
    "- There's a linear relationship between the response variable and the explanatory variables.\n",
    "- There's no pattern in the residual plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you're a master of simple linear regression, you're probably thinking \"WHY CAN'T I USE MORE EXPLANATORY VARIABLES? What if I think `mpg` could be better predicted if I knew *two* of the variables? Wouldn't that make my model better?\" Why, Ms/Mr. Genius Statistician, you *can* use more explanatory variables! That leads us to *multiple linear regression*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multiple'></a>\n",
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiple linear regression** is an extension to the simple linear regression model with multiple explanatory variables instead of just one.\n",
    "\n",
    "With two explanatory variables, we can still visualize the model in a three-dimensional graph, but as we add more and more variables it's pretty much impossible to plot it (can you imagine what a 5D graph would look like?). \n",
    "\n",
    "Below is a code chunk that plots the scatterplots of `weight` and `displacement` against `mpg`, as well as the corresponding linear model. It's interactive, so you can drag it around to get a better look at how the model fits the data!\n",
    "\n",
    "You'll notice that the model is no longer a line â€“ it's a plane. This is the 3D analog to a line. Just as a line defines one value for $y$ for any given $x$, a plane defines one value for $z$ for any pair of $(x, y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plot_multiple_linear_regression(mpg_train, \"displacement\", \"weight\", \"mpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_model = LinearRegression()\n",
    "X3 = ... # select both the displacement and weight columns from mpg_train\n",
    "multiple_model.fit(X3, Y)\n",
    "\n",
    "print(\"Multiple Linear Regression R^2:\", multiple_model.score(X3, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lookin' good! Let's compare this $R^2$ value with the $R^2$ of the simple linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispX = ... # select just displacement from mpg_train\n",
    "wtX = ... # select just weight from mpg_train\n",
    "\n",
    "linear_model.fit(dispX, Y)\n",
    "print(\"Simple Linear Regression (displacement) R^2:\", linear_model.score(dispX, Y))\n",
    "\n",
    "linear_model.fit(wtX, Y)\n",
    "print(\"Simple Linear Regression (weight) R^2:\", linear_model.score(wtX, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What do you notice about the $R^2$ values of the simple linear regression models and the multiple case?\n",
    "\n",
    "**Question 2**: Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fitting'></a>\n",
    "# Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='underfitting'></a>\n",
    "## Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by now you might be thinking, how do I know when to use simple linear regression or multiple linear regression? Let's take a look again at the simple linear regression model using `displacement` as our explanatory variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "X = mpg_train[[\"displacement\"]]\n",
    "\n",
    "linear_model.fit(X, Y)\n",
    "overlay_simple_linear_model(mpg_train, 'displacement', 'mpg', linear_model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that the simple linear model doesn't really fit the data that well. The line of best fit clearly misses out on a good chunk of the points towards the middle of the scatter plot, so if we had to use this line to predict `mpg` for new values of `displacement`, our model might not make the best predictions.\n",
    "\n",
    "When a model is too simple to accurately describe the relationship between variables, we say that it is **underfitting**.\n",
    "\n",
    "We can also take a look at the residual plot of this linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_simple_residuals(mpg_train, 'displacement', 'mpg', linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What do you notice about the residual plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The residual plot shows us that we're overpredicting for the intermediate values for `displacement` and underpredicting for the higher values of `displacement`. This suggests that a different model may be better suited for this data, since our model is not accurately capturing the shape of our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So, how do we fix this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_transformation'></a>\n",
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to fix this error in a best fit line is to transform our explanatory variable! You might think that the shape of the scatterplot could be modeled better by a model of the form $y = a\\log{x} + b$.  We can preliminarily test whether it would make sense to make such a model by looking at the scatterplot comparing `mpg` and `log(displacement)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.log(mpg_train['displacement']), mpg_train['mpg'])\n",
    "plt.title(\"Scatter Plot of log(displacement) vs. mpg\")\n",
    "plt.xlabel(\"log(displacement)\")\n",
    "plt.ylabel(\"mpg\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, transforming our data can allowed us to see a more linear relationship between $log(displacement)$ and $mpg$, so let's make a linear regression model based off of that! We can do this by transforming our $x$ into $\\log{x}$ and then running linear regression in the same way we did before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logarithmic_model = LinearRegression()\n",
    "X = mpg_train[[\"displacement\"]]\n",
    "\n",
    "transformedX = np.log(X)\n",
    "logarithmic_model.fit(transformedX, Y)\n",
    "\n",
    "x = np.arange(min(X.values), max(X.values)).reshape((-1, 1))\n",
    "transformedx = np.log(x)\n",
    "predicted = logarithmic_model.predict(transformedx) # predicted values\n",
    "\n",
    "mpg_train.plot.scatter(\"displacement\", \"mpg\")\n",
    "plt.plot(x, predicted, color='red')\n",
    "plt.title(\"Logarithmic Model: displacement vs. mpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the residuals for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = Y - logarithmic_model.predict(transformedX)\n",
    "\n",
    "plt.scatter(X, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.title(\"Residual Plot: log(displacement) vs. mpg\")\n",
    "plt.xlabel(\"displacement\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='polynomial'></a>\n",
    "### Polynomial Regression\n",
    "Alternatively, you might have thought the curve of the line reminded you of a **quadratic** function of the form $y = ax^2 + bx + c$. We can make a model of this form by simply doing multiple linear regression with our two explanatory variables as $x$ and $x^2$, which we can make by transforming $x$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_model = LinearRegression()\n",
    "X = mpg_train[[\"displacement\"]]\n",
    "\n",
    "X_squared = np.power(X, 2)\n",
    "combinedX = np.hstack((X, X_squared)) # we're horizontally stacking X_squared, because we're adding columns\n",
    "polynomial_model.fit(combinedX, Y)\n",
    "\n",
    "x = np.arange(min(X.values), max(X.values)).reshape((-1, 1))\n",
    "x_squared = np.power(x, 2)\n",
    "combinedx = np.hstack((x, x_squared))\n",
    "predicted = polynomial_model.predict(combinedx) # predicted values\n",
    "\n",
    "mpg_train.plot.scatter(\"displacement\", \"mpg\")\n",
    "plt.plot(x, predicted, color='red') \n",
    "plt.title(\"Quadratic Model: displacement vs. mpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = Y - polynomial_model.predict(combinedX)\n",
    "\n",
    "plt.scatter(X, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.title(\"Residual Plot: (displacement)^2 and displacement vs. mpg\")\n",
    "plt.xlabel(\"displacement\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the residual plot shows points much more randomly scattered around the line $y=0$, and we don't see any patterns that don't resemble a random scatter in this plot. This model is clearly much better than the simple linear model, as depicted visually through seeing the 'fit' of the newer model on the dataset and through the random scatter in the residual plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly in this example, raising the complexity of our model allowed us to better fit our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overfitting'></a>\n",
    "## Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why stop at $y = ax^2 + bx + c$? Why not go even further and make a degree-15 polynomial model of the form $y = ax^{15} + bx^{14} + ... + c$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_stack(X, degrees):\n",
    "    \"\"\"\n",
    "    This function creates a matrix in which each column k contains X^(k+1), e.g. column 0 has X and column 3 has X^4\n",
    "    \n",
    "    X(array): an array representing a single column of data\n",
    "    degrees: the highest degree polynomial to return\n",
    "    \n",
    "    returns a np.array of shape (len(X), degrees)\n",
    "    \"\"\"\n",
    "    return np.hstack((np.power(X, i+1) for i in range(degrees)))\n",
    "    \n",
    "def polynomial_regression(data, x_name, y_name, degrees):\n",
    "    \"\"\"\n",
    "    This function plots a polynomial regression model overlaid on a scatterplot of the original data.\n",
    "    \n",
    "    data(DataFrame): e.g. mpg_train\n",
    "    x_name(string): the name of the column representing the predictor variable\n",
    "    y_name(string): the name of the column representing the dependent/response variable\n",
    "    linear_model\n",
    "    degrees(int): the degree of the polynomial to fit the model on\n",
    "    \n",
    "    returns the polynomial regression model\n",
    "    \"\"\"\n",
    "    X = data[[x_name]]\n",
    "    Y = data[[y_name]]\n",
    "    X, Y = X/1000., Y\n",
    "    poly = LinearRegression()\n",
    "    X_stack = polynomial_stack(X, degrees)\n",
    "    poly.fit(X_stack, Y)\n",
    "\n",
    "    x = np.arange(min(X.values), max(X.values)+.001, .001).reshape((-1, 1))\n",
    "    x_stack = polynomial_stack(x, degrees)\n",
    "    predicted = poly.predict(x_stack)\n",
    "\n",
    "    data.plot.scatter(x_name, y_name)\n",
    "    plt.plot(1000*x, predicted, color='red')\n",
    "    plt.title(\"Polynomial Degree \" + str(degrees) + \" Model: \" + x_name + \" vs. \" + y_name)\n",
    "    plt.show()\n",
    "    return poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "poly_3 = polynomial_regression(mpg_train, \"displacement\", \"mpg\", 3)\n",
    "poly_15 = polynomial_regression(mpg_train, \"displacement\", \"mpg\", 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we cover many more points than we did before with the simple linear regression (x vs y plot). **So why not keep doing this and make more and more complex regression lines (best-fit lines)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refrain from making more and more complex regression lines because these lines are not only used to fit the sample we fit the model on, but we want to use it as a general descriptor of the relationship between these variables. If we wanted to use it to **predict** future points, it wouldn't work that well for most values. Looking at our last model, do you think it accurately describes the `mpg` of a car with `displacement = 75`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can imagine how this kind of discrepancy might be the cuase for error in other datasets when we try to get a line of best fit, as in those cases, we may not always know what is nonsensical output. For example, the dataset may not have as clear features as `displacement` and `mpg`, and as a result, you might not be able to determine when you've predicted an output that doesn't make any sense in the context of the data. Or maybe you won't be able to visualize it as easily as we can with only one explanatory variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of situation is what we would call **overfitting** our model on our training data. Even though we can make a model fit perfectly on our training data, it doesn't generalize well with other data points, as a model is supposed to show a **generalization of reality**. The more complex model we used to fit our model this time isn't necessarily reflective of what we see happens in real world phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='r_squared_overfitting'></a>\n",
    "### $R^2$ and Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit the $R^2$ coefficient we talked about earlier today. We used it as a method of assessing the quality of our model, since it measures how well our model fits the data. Let's take a look at the $R^2$ values of the polynomial models from the section above; what do you notice about them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mpg_train[[\"displacement\"]]\n",
    "X_stack_3 = polynomial_stack(X/1000, 3)\n",
    "print(\"Degree 3 Polynomial R^2: \" + str(poly_3.score(X_stack_3, Y)))\n",
    "\n",
    "X_stack_15 = polynomial_stack(X/1000, 15)\n",
    "print(\"Degree 15 Polynomial R^2: \" + str(poly_15.score(X_stack_15, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the $R^2$ values, the higher degree of a polynomial we model our data on, the higher the $R^2$ value seems to be. However, if we recall from before, the definition of the $R^2$ value is based off of how well our model predicts the variation in our data. So, the closer our $R^2$ value is to 1, the more variation from our data is accounted in our model, meaning our model can easily be overfitting on our data (If we take into account nearly all the variation in our data, that must mean that our model is passing through very closely to all points). This doesn't necessarily imply that our model is overfitting on our data, as points in a straight line will have $R^2 = 1$ even with a simple linear model, but this is just to show that $R^2$ is sensitive to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One value that exists to act as a measure of penalizing overfitting is the **adjusted $R^{2}$ value**. Now we won't be going into how it works in this particular lecture, but click [here](http://blog.minitab.com/blog/adventures-in-statistics-2/multiple-regession-analysis-use-adjusted-r-squared-and-predicted-r-squared-to-include-the-correct-number-of-variables) if you're interested in learning more about what it is! For now, we're just going to keep the focus more on the $R^{2}$ value, as this is the one more commonly used in real-world statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to recap, the $R^2$ of a model can be a misleading measure of how well your model is doing! Just looking at the $R^2$ value of a model on its data can make you think the model is doing really well, when in reality, the model is just overfitting on the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overfitting_demo'></a>\n",
    "### Overfitting Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at what overfitting looks like when looking at varying degrees of models on the same data points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "overfittingDemo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, as we our model's degree beyond a degree 3 model, our predicted model becomes more and more erratic, which isn't necessarily how the true data is distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know lines of best fit can underfit and overfit our data, we may think we're in a bit of a predicament! **How can we determine what line of best fit best fits our data and generalizes the best with the world?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is, it's completely relative to your data set! There's no magic number for the maximum degree polynomial you can have when plotting your line of best-fit (though there can be extremes) and the same for the mimumum. However, a general trend for fitting follows something simliar to this chart:\n",
    "\n",
    "<img src='trueError.png' width=\"400\" height=\"400\">\n",
    "\n",
    "As we increase in the model order (the max degree polynomial we use as a best fit line for our data), we see that the true error for our graph dips at a certain point, but continuously increases over time after that. What we're trying to do is determine a best-fit line that hovers as close as possible to the point of lowest true error.\n",
    "\n",
    "**But what's a good strategy to find that?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something we call **Validation**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='validation'></a>\n",
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've talked a lot about what underfitting and overfitting are, and how to visually detect whether or not your model is doing one or the other. However, we haven't gone into any strategies into how we can achieve some kind of happy medium between underfitting and overfitting, so our model fits well against **the underlying distribution of our data** and also **generalizes well against other data points**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know how to accomplish the first part â€“ by running linear regression and evaluating $R^2$, while potentially adding more explanatory variables, but how do we accomplish the second part? How can we check how well our model does on points it hasn't been trained on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we'd be able to just get more data to test whether our model is overfitting on our data, but that's not always possible. So, we instead separate the data that we have into different sets. In order to figure out what the best model is, we can train various models on a subset of our data, and then have some data set aside that we can evaluate each of our potential models against. The data we're training our models on is called the **training set**, while the data we evaluate our models on is the **validation set**. Once we've chosen the model that minimizes the error on our *validation set*, we can perform a final test of how good our model is on data that we've *never* seen before, the **test set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that might be a bit confusing to grasp at first, so we're going to break it down!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order to verify how well our model is doing, we can split our initial data set that we're given into three different groups:\n",
    "* **Training set:** The set of data points you use to fit your model.\n",
    "* **Validation set:** The set of data points that you use to fine tune your model's parameters (aka \"fine tuning your model's hyperparameters\")\n",
    "    * Hyperparameters are essentially the parameters your model uses, which in our case could be which degree polynomial to use\n",
    "* **Test set:** The set of data points that you use to evaluate how well your model that was fit against your training set does against other \"outside\" data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find that the terms **validation set** and **test set** are sometimes used interchangably in outside works concerning testing how accurate your model is, but we'll be using the formal definitions and uses of all 3 of the sets listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, instead of getting new data points to test our model against, we just split our initial data set into three groups- typically done in a 60/20/20 split ($60\\%$ of data to training, $20\\%$ to validation, and $20\\%$ to testing). Here's a little visual of what we're trying to convey:\n",
    "\n",
    "<img src=\"train-validate-test.png\" width=400 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're essentially just splitting up our dataset into 3 distinct datasets for future usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this splitting is done, we're able to follow the following steps through a process of validating the best model for your dataset.\n",
    "1. Initially fit a model against our **training set**.\n",
    "2. Use the **validation set** to provide information on which hyperparameters might be best tailored for your model (in our case, which coefficients and which degree polynomial might be the best to use) by repeatedly testing different models against this dataset. At the end, choose whichever hyperparameters for your model that are the most favorable for you (in our case, the most favorable hyperparamters are the ones that reduce as much loss as possible, but this won't necessarily always be the case).\n",
    "3. Take the favorable model from step 2 and test the model against our **test set**. The result will be an unbiased indicator of how well our model does with \"generalizing against other data points\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='error'></a>\n",
    "### Training Error and Test Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, training a model, validating which hyperparameters are the best suited for our model, and then finally testing it to see it's final accuracy are all steps we can follow to create a model that no longer **underfits** or **overfits** on our data. So, how do we define which model performs the best against the validation set? By choosing the model that minimizes the error on the validation set! To begin, let's define a few terms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Training error:** The error returned from running an error function on your model with your training set. In our case, we determine the MSE (mean squared error) from the data points in our training set to our model.\n",
    "* **Test error:** The error returned from running an error function on your fine-tuned post-validation model with your test set. In our case, we still use the MSE to determine how accurate our model is doing against data points in our test set.\n",
    "* **Mean Squared Error:** The error output from finding the average of all the squared magnitudes from all the predicted and actual outputs. This is the type of error \"function\" we use to calculate training and test errors. To get a better idea of what **MSE** is, take a look at the following picture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='mean_squared_error.png' width=400, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, these numerical values for **training error** and **test error** can also be measurements through which we can determine how well our model is doing on our data points. You can imagine that if we wanted to lower our training error, we'd want to better fit a model against our training points. However, as we talked about before, the more we try to fit a model against a dataset, the more likely we are to **overfit**. So, minimizing our **training error** isn't necessarily what we're typically trying to achieve. A good visualization that conveys what we're talking about is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='training_vs_test_error.png' width=\"800\" height=\"800\">\n",
    "\n",
    "As we increase in the model order (the max degree polynomial we use as a best fit line for our data), we can see that we lower our **training error**, which can be good in the beginning. We see that our test error is also minimized up until our model reaches degree 3 or so. However, as we continue to lower our **training error**, we see that our **test error** continues to increase. Now, we can see that we're beginning to **overfit** on our training data, and **not generalizing well** against other data points (in this case, our test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why do we conduct this entire process, and not just repeated test different models against our test set until we get the lowest error possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we don't continuously test against our test set is because that would not only result in a biased evaluation of how well our model is doing, but also because our models would be able to **overfit** by repeatedly trying to lower our error from testing it on our test set. We want to prevent our model from **overfitting** on any data set, including the test set, as the test set is just something we made to conduct tests from our original data. There are limitless possibilities to what our test set could've been like, and so, there can be enough unpredicability within our test set that other test sets we could've made could produce totally different results. So, we try to refrain from testing repeatedly on the test set, and rather just test our final model on the test set to have a final verification on how well it's able to predict and generalize to other data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In conclusion**, we can see that validation can be an incredibly useful process through which we create a model that fits, but doesn't overfit or underfit, on our data. We also see that different measurable quantities, such as training error and test error, can also be indicative of how well our model is doing at different periods of time, and see how well it's fitting against our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a demo with this on the data we've been using from the `mpg` dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation = mpg_validation[[\"displacement\"]]\n",
    "Y_validation = mpg_validation[[\"mpg\"]]\n",
    "X_validation_stack_3 = polynomial_stack(X_validation/1000, 3)\n",
    "pred_3 = poly_3.predict(X_validation_stack_3)\n",
    "print(\"Degree 3 Polynomial validation error: \" + str(mean_squared_error(Y_validation, pred_3)))\n",
    "\n",
    "X_validation_stack_15 = polynomial_stack(X_validation/1000, 15)\n",
    "pred_15 = poly_15.predict(X_validation_stack_15)\n",
    "print(\"Degree 15 Polynomial validation error: \" + str(mean_squared_error(Y_validation, pred_15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the degree 3 polynomial has lower error based on our validation set, so we will pick that as our final model of the two and evaluate it on our test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = mpg_test[[\"displacement\"]]\n",
    "Y_test = mpg_test[[\"mpg\"]]\n",
    "X_test_stack_3 = polynomial_stack(X_test/1000, 3)\n",
    "pred_3 = poly_3.predict(X_test_stack_3)\n",
    "print(\"Degree 3 Polynomial test error: \" + str(mean_squared_error(Y_test, pred_3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='recap'></a>\n",
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to recap, here's a great visual that covers a lot of what we just went over in the Underfitting, Overfitting, and Validation sections in this notebook!\n",
    "\n",
    "<img src='fit_graphs.png' width=\"1600\" height=\"1600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You've learning the concepts of linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exercises'></a>\n",
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how to train models on pieces of data, how to validate the model with a validation dataset, and how to test how well your chosen model is doing against a test dataset, we want you to construct some models/visualizations on the `mpg_train` dataset we've been working with! A quick tip we have for solving these problems: open up another window of this same notebook and scroll through that one so that you don't have to keep scrolling up and down on this one to fill out answers for functions that you might've forgotten how to use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Let's try to determine a correlation between two different variables in the `mpg` dataset! First, let's split the `mpg` dataset into a training set, validation set, and a test set. We didn't really go into the particular function that does this for you in this notebook, but you can scroll up and see how it's used (it's called `train_test_split`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Now reconstruct the following scatter plot from the `mpg_train1` dataset.\n",
    "\n",
    "<img src='exer_2_pic.png' width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Draw a simple linear model through the graph you made right above. It should look something like what we've displayed below. *Hint: you can use the `overlay_simple_linear_model()` function*\n",
    "\n",
    "<img src='exer_3_pic.png' width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** What are the coefficients and intercept for the model you used above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your return the value of the coefficient, \n",
    "# not a list with the coefficient in it\n",
    "coefficient = ...\n",
    "intercept = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(coefficient, -0.16438784)\n",
    "assert np.isclose(intercept, 40.2508769656186)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:** Find the residual plot for the model and datapoints you plotted in the exercise above. The outcome should look something like the following:\n",
    "\n",
    "<img src='exer_5_pic.png' width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** What does the above scatter plot say about the correlation between the `horsepower` of a car and the `mpg` of the car? How well does the model fit against the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** Calculate the $R^{2}$ value between the two variables ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_value = ...\n",
    "R2_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.isclose(R2_value, 0.6217949606638364)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've taken a look at what the data looks like, and ran a simple linear regression through it, let's conduct the steps of Validation to determine what's the best model for this dataset (**the one that generalizes the best and the one that covers the underlying trend between the two variables, not the noise**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:** Fit a variety of different models against the `mpg_train1` dataset to determine the model that best depicts the relationship between `horsepower` and `mpg`. Both **validation errors** and data visualizations can be good indicators in determining which regression model best fits your data. *(Hint: Refer to the functions `polynomial_regression()` and `polynomial_stack()` earlier in the notebook).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Creating and testing different models ###\n",
    "\n",
    "# A for loop might be really useful here to visualize models and training errors for different degree models\n",
    "\n",
    "model_x, model_y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9:** Basing off of the plots above and their respective training errors, which degree model did you determine was the best suited for your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert answer in range(1, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10:** Final question: now that we've verified which model fits our data the best, let's take a look at the **testing error** of the model against our test set. *(Hint: you will probably have to re-predict the model with `polynomial_regression()` and `polynomial_stack()`)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You've finished the exercises! You are officially a linear regression master!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (susa)",
   "language": "python",
   "name": "susa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
