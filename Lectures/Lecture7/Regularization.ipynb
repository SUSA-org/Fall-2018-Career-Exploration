{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toyota corolla</th>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>108</td>\n",
       "      <td>70</td>\n",
       "      <td>2245</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buick century</th>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>231</td>\n",
       "      <td>110</td>\n",
       "      <td>3907</td>\n",
       "      <td>21</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cadillac eldorado</th>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>350</td>\n",
       "      <td>125</td>\n",
       "      <td>3900</td>\n",
       "      <td>17</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmw 320i</th>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>121</td>\n",
       "      <td>110</td>\n",
       "      <td>2600</td>\n",
       "      <td>12</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ford fairmont futura</th>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>140</td>\n",
       "      <td>92</td>\n",
       "      <td>2865</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      mpg  cylinders  displacement  horsepower  weight  \\\n",
       "name                                                                     \n",
       "toyota corolla         34          4           108          70    2245   \n",
       "buick century          17          6           231         110    3907   \n",
       "cadillac eldorado      23          8           350         125    3900   \n",
       "bmw 320i               21          4           121         110    2600   \n",
       "ford fairmont futura   24          4           140          92    2865   \n",
       "\n",
       "                      acceleration  model_year  origin  \n",
       "name                                                    \n",
       "toyota corolla                  16          82       3  \n",
       "buick century                   21          75       1  \n",
       "cadillac eldorado               17          79       1  \n",
       "bmw 320i                        12          77       2  \n",
       "ford fairmont futura            16          82       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg = pd.read_csv(\"./mpg.csv\", index_col=\"name\") # load mpg dataset\n",
    "mpg = mpg.loc[mpg[\"horsepower\"] != '?'].astype(int) # remove columns with missing horsepower values\n",
    "mpg_train, mpg_test = train_test_split(mpg, test_size = .2, random_state = 0) # split into training set and test set\n",
    "mpg_train, mpg_validation = train_test_split(mpg_train, test_size = .5, random_state = 0)\n",
    "mpg_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regularization'></a>\n",
    "## Regularization\n",
    "\n",
    "Last week we talked about validation as a means of combating overfitting. Another method is to add *regularization* terms to our loss function. **Regularization** basically penalizes complexity in our models. This allows us to add explanatory variables to our model without worrying as much about overfitting. Here's what our ordinary least squares model looks like with a regularization term:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\arg\\!\\min_\\theta \\sum_{i=0}^n (y_i - f_\\boldsymbol{\\theta}(x_i))^2 + \\lambda R(\\boldsymbol{\\theta})$$\n",
    "\n",
    "We've written the model a little differently here, but the first term is the same as the ordinary least squares regression model you learned last week. This time it's just generalized to any function of $x$ where $\\theta$ is a list of parameters, or weights on our explanatory variables, such as coefficients to a polynomial. We're minimizing a loss function to find the best coefficients for our model. \n",
    "\n",
    "The second term is the **regularization** term. The $\\lambda$ parameter in front of it dictates how much we care about our regularization term – the higher $\\lambda$ is, the more we penalize large weights, and the more the regularization makes our weights deviate from OLS. \n",
    "\n",
    "**Question**: What happens when $\\lambda = 0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is $R(\\theta)$? It could be a lot of things! Today we'll talk about two of the most common regularization functions – ridge and LASSO. \n",
    "\n",
    "<table>\n",
    "    <tr><td>Ridge</td><td>L2 Norm</td><td>$R(\\boldsymbol{\\theta}) = \\sum\\limits_{i=0}^n \\theta_i^2$</td></tr>\n",
    "    <tr><td>LASSO</td><td>L1 Norm</td><td>$R(\\boldsymbol{\\theta}) = \\sum\\limits_{i=0}^n \\lvert\\theta_i\\rvert$</td></tr>\n",
    "</table>\n",
    "\n",
    "<a id='ridge'></a>\n",
    "### Ridge \n",
    "In **ridge** regression, the regularization function is the sum of squared weights. One of the nice things about ridge regression is that there is always a unique, mathematical solution that can be found using a known formula. The solution involves linear algebra, so you don't need to know it, but the existence of this formula also makes it computationally easy to solve.\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\left(\\boldsymbol{X}^T \\boldsymbol{X} + \\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{Y}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "x_train = ...\n",
    "y_train = ...\n",
    "\n",
    "ridge_model = Ridge(alpha = 1.0)\n",
    "ridge_model.fit(x_train, y_train)\n",
    "loss = ... # mean squared error or ridge model\n",
    "print(\"Root Mean Squared Error of ridge model: {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lasso'></a>\n",
    "### LASSO\n",
    "In **LASSO** regression, the regularization function is the sum of absolute values of the weights. If there's one thing you should know about LASSO is that it is *sparsity inducing*. This just means that it forces some weights to take on zero values, leaving you with fewer explanatory variables in the resulting model than you put in. Unlike ridge regression, LASSO doesn't necessarily have a unique solution, and there's no formula that determines what the optimal weights should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_model = Lasso(alpha = 1.0)\n",
    "lasso_model.fit(x_train, y_train)\n",
    "\n",
    "loss = ...\n",
    "print(\"Root Mean Squared Error of lasso model: {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Ridge and LASSO\n",
    "We just told you a lot of things about ridge and lasso, but here are some visualizations to help you understand the intuition behind some of the characteristics of these two regularization methods. Another way to describe the modified minimization function above is that it's the same loss function as before, with the *additional constraint* that $R(\\boldsymbol{\\theta}) \\leq t$. Now, $t$ is related to $\\lambda$ but the exact relationship between the two parameters depends on your data. Regardless, let's take a look at what this means in the two-dimensional case. For ridge,\n",
    "\n",
    "$$\\theta_0^2 + \\theta_1^2 \\leq t$$\n",
    "\n",
    "Does this look familiar to you? What if it's in the form $x^2 + y^2 \\leq t$? Or how about now:\n",
    "<img src='http://vikingsseason5i.com/wp-content/uploads/2018/08/circle-equation-circle-equation-unit-circle.jpg' width=400 />\n",
    "\n",
    "Lasso is of the form $$\\left|\\theta_0\\right| + \\left|\\theta_1\\right| \\leq t$$ This one's a little harder to interpret, perhaps this will help inspire you:\n",
    "<img src='https://cdn.kastatic.org/ka-perseus-graphie/3b9b8f4b4dac19e1197e9dd94553d0822f9fe69a.png' />\n",
    "\n",
    "#### Norm Balls\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/f/f8/L1_and_L2_balls.svg' width=400/>\n",
    "<img src='norm_balls.png' width=400/>\n",
    "\n",
    "The rhombus and circle as a visualization of the regularization term, while the blue circles are the topological curves representing the loss function based on the weights. You want to minimize the sum of these, which means you want to minimize each of those. The point that minimizes the sum is the minimum point at which they intersect.\n",
    "\n",
    "\n",
    "**Question**: Based on these visualizations, could you explain why LASSO is sparsity-inducing?\n",
    "\n",
    "Turns out that the $L2-norm$ is always some sort of smooth surface, from a circle in 2D to a sphere in 3D. On the other hand, LASSO always has sharp corners. This is exactly the feature that makes it sparsiy-inducing. As you might imagine, just as humans are more likely to bump into sharp corners than smooth surfaces, the loss term is also most likely to intersect the $L2-norm$ at one of the corners.\n",
    "\n",
    "### Regularization and Bias Variance\n",
    "As we mentioned earlier, the bias is the average OLS loss term across multiple models of the same family (e.g. same degree polynomial) trained on separate datasets. Variance is the average variance of the weight vectors (coefficients) on your features. \n",
    "\n",
    "Without the regularization term, we’re just minimizing bias; the regularization term means we won’t get the lowest possible bias, but we’re exchanging that for some lower variance so that our model does better at generalizing to data points outside of our training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check\n",
    "\n",
    "1. What happens as $\\lambda$ increases?\n",
    "    1. bias increases, variance increases\n",
    "    2. bias increases, variance decreases\n",
    "    3. bias decreases, variance increases\n",
    "    4. bias decreases, variance decreases\n",
    "\n",
    "\n",
    "2. **True** or **False**? Bias is how much error your model makes.\n",
    "\n",
    "\n",
    "3. What is **sparsity**?\n",
    "\n",
    "\n",
    "4. For each of the following, choose **ridge**, **lasso**, **both**, or **neither**:\n",
    "    1. L1-norm\n",
    "    2. L2-norm\n",
    "    3. Induces sparsity\n",
    "    4. Has analytic (mathematical) solution\n",
    "    5. Increases bias\n",
    "    6. Increases variance\n",
    "    7. Results in lowest possible loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (susa)",
   "language": "python",
   "name": "susa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
