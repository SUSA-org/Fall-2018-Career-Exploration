{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: Bias-Variance Tradeoff, Regularization\n",
    "## 10/23/18\n",
    "\n",
    "### Table Of Contents\n",
    "1. [Recap](#recap)  \n",
    "2. [Bias-Variance Tradeoff](#bv-tradeoff)\n",
    "3. [Regularization](#regularization)  \n",
    "\n",
    "\n",
    "### Hosted by and maintained by the [Statistics Undergraduate Students Association (SUSA)](https://susa.berkeley.edu). Authored by [Ajay Raj](mailto:araj@berkeley.edu), [Nichole Sun](mailto:nicholesun@berkeley.edu), [Rosa Choe](mailto:rosachoe@berkeley.edu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from plotting import overfittingDemo, plot_multiple_linear_regression, ridgeRegularizationDemo, lassoRegularizationDemo\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='recap'></a>\n",
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](fit_graphs.png \"Fit Graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias corresponds to underfitting. If we look at the first model, the points seem to follow some sort of curve, but our predictor is linear and therefore, unable to capture all the points. In this case, we have chosen a model which is not complex enough to accurately capture all the information from our data set. \n",
    "\n",
    "If we look at the last model, the predictor is now overly complex because it adjusts based on every point in order to get as close to every data point as possible. In this case, the model changes too much based on small fluctuations caused by insignificant details in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bv-tradeoff'></a>\n",
    "## Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll perform **model evaluation**, where we'll judge how our linear regression models actually perform. Last week, you talked about a **loss function**, which describes a numerical value for how far your model is from the true values.\n",
    "\n",
    "$$\\textit{RSS} = \\sum_{i=0}^n {e_i}^2 = \\sum_{i=0}^n (y_i - mx_i - b)^2$$\n",
    "\n",
    "We'll change it up a little: Say that there are $p$ features, or independent variables. Your task is to create a model $\\hat{f}$, such that the loss is now:\n",
    "\n",
    "$$\\frac{1}{n} \\sum_i^n (y_i - \\hat{f}(x_i))^2$$\n",
    "\n",
    "In this loss function, $y_i$ is a number, and $x_i$ is a $p$-vector, because there are $p$ features. This loss is called **mean squared error**, or **MSE**.\n",
    "\n",
    "Now, we'll talk about other ways to evaluate a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define some terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that everything in the universe can be described with the following equation:\n",
    "\n",
    "$$y = h(x) + \\epsilon$$\n",
    "\n",
    "- $y$ is the quantity you are trying to model\n",
    "- $x$ are the parameters (independent variables)\n",
    "- $h$ is the **true model** for $y$ in terms of $x$\n",
    "- $\\epsilon$ represents **noise**, a random number which has mean zero\n",
    "\n",
    "Let $\\hat{f}$ be your model for $y$ in terms of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Bias\n",
    "\n",
    "When evaluating a model, the most intuitive first step is to look at how well the model performs. For classification, this may be the percentage of data points correctly classified, or for regression it may be how close the predicted values are to actual. The **bias** of a model is a measure of how close our prediction is to the actual value on average from an average model. Note that bias is not a measure of a single model, it encapuslates the scenario in which we collect many datasets, create models for each dataset, and average the error over all of models. Bias is not a measure of error for a single model, but a more abstract concept describing the average error over all errors. A low value for the bias of a model describes that on average, our predictions are similar to the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "The **variance** of  a model relates to the variance of the distribution of all models. In the previous section about bias, we envisoned the scenario of collecting many datasets, creating models for each dataset, and averaging the error overall the datasets. Instead, the variance of a model describes the variance in prediction. While we might be able to predict a value very well on average, if the variance of predictions is very high this may not be very helpful, as when we train a model we only have one such instance, and a high model variance tells us little about the true nature of the predictions. A low variance describes that our model will not predict very different values for different datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](BiasVariance.jpg \"Bias Variance Visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image describes what bias and variance are in a more simplified example. Consider that we would like to create a model that selects a point close to the center. The models on the top row have low bias, meaning the center of the cluster is close to the red dot on the target. The models on the left column have low variance, the clusters are quite tight, meaning our predictions are close together.\n",
    "\n",
    "What is the order of best scenarios?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are trying to minimize **expected error**, or the average **MSE** over all datasets. It turns out (with some advanced probability gymnastics), that:\n",
    "\n",
    "$$\\text{Expected Error} = \\text{Noise Variance} + \\text{Bias}^2 + \\text{Variance}$$\n",
    "\n",
    "Note that $\\text{Noise Variance}$ is constant: we assume there is some noise, and $\\text{Noise Variance}$ is simply a value that describes how noisy your dataset will be on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation defines what is known as the **bias variance tradeoff**. \n",
    "\n",
    "![alt text](BiasVarianceTradeoff.png \"Bias Variance Tradeoff\")\n",
    "\n",
    "Image from http://scott.fortmann-roe.com/docs/BiasVariance.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this true intuitively?\n",
    "\n",
    "At some point as we decrease **bias**, instead of getting closer to the **true model** $h$, we go past and try to fit to the $\\epsilon$ (noise) that is part of our current dataset. This is equivalent to making our model more noisy: which means that over all datasets, it has more **variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions for understanding**:\n",
    "> 1. Where does underfitting and overfitting lie in the graph above? How do they relate to bias and variance?\n",
    "> 2. Why can't we usually just make a bunch of models with low bias and high variance and average them?\n",
    "> 3. Why is low variance important in models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "Let's revisit the polynomial problem from last week.\n",
    "\n",
    "In this case, if our model has degree $d$, we have $d + 1$ features: $x = [x^0, x^1, ..., x^d]$. Now, we have a linear model with $d + 1$ features:\n",
    "\n",
    "$$\\hat{f}(x) = \\sum_{i=0}^{d} a_i x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model complexity in this case is the degree of the polynomial. As we saw last week, as $d$ increases, model complexity increases. The model gets better, but then gets erratic. This directly corresponds to the bias-variance graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "overfittingDemo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw last time, the best model was a degree 3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = pd.read_csv(\"mpg.csv\", index_col='name')# load mpg dataset\n",
    "mpg = mpg.loc[mpg[\"horsepower\"] != '?'].astype(float) # remove columns with missing horsepower values\n",
    "mpg_train, mpg_test = train_test_split(mpg, test_size = .2, random_state = 0) # split into training set and test set\n",
    "mpg_train, mpg_validation = train_test_split(mpg_train, test_size = .5, random_state = 0)\n",
    "mpg_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regularization'></a>\n",
    "## Regularization\n",
    "\n",
    "Last week we talked about validation as a means of combating overfitting. Another method is to add *regularization* terms to our loss function. **Regularization** basically penalizes complexity in our models. This allows us to add explanatory variables to our model without worrying as much about overfitting. Here's what our ordinary least squares model looks like with a regularization term:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\arg\\!\\min_\\theta \\sum_{i=0}^n (y_i - f_\\boldsymbol{\\theta}(x_i))^2 + \\lambda R(\\boldsymbol{\\theta})$$\n",
    "\n",
    "We've written the model a little differently here, but the first term is the same as the ordinary least squares regression model you learned last week. This time it's just generalized to any function of $x$ where $\\theta$ is a list of parameters, or weights on our explanatory variables, such as coefficients to a polynomial. We're minimizing a loss function to find the best coefficients for our model. \n",
    "\n",
    "The second term is the **regularization** term. The $\\lambda$ parameter in front of it dictates how much we care about our regularization term – the higher $\\lambda$ is, the more we penalize large weights, and the more the regularization makes our weights deviate from OLS. \n",
    "\n",
    "**Question**: What happens when $\\lambda = 0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is $R(\\theta)$? It could be a lot of things! Today we'll talk about two of the most common regularization functions – ridge and LASSO. \n",
    "\n",
    "<table>\n",
    "    <tr><td>Ridge</td><td>L2 Norm</td><td>$R(\\boldsymbol{\\theta}) = \\sum\\limits_{i=0}^n \\theta_i^2$</td></tr>\n",
    "    <tr><td>LASSO</td><td>L1 Norm</td><td>$R(\\boldsymbol{\\theta}) = \\sum\\limits_{i=0}^n \\lvert\\theta_i\\rvert$</td></tr>\n",
    "</table>\n",
    "\n",
    "<a id='ridge'></a>\n",
    "### Ridge \n",
    "In **ridge** regression, the regularization function is the sum of squared weights. One of the nice things about ridge regression is that there is always a unique, mathematical solution that can be found using a known formula. \n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\left(\\boldsymbol{X}^T \\boldsymbol{X} + \\lambda\\boldsymbol{I}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{Y}$$\n",
    "\n",
    "If you recall, the solution to linear regression was of the form:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}} = \\left(\\boldsymbol{X}^T \\boldsymbol{X}\\right)^{-1}\\boldsymbol{X}^T\\boldsymbol{Y}$$\n",
    "\n",
    "And we said that the $\\boldsymbol{X}^T\\boldsymbol{X}$ isn't always invertible. What about $\\boldsymbol{X}^T \\boldsymbol{X} + \\lambda\\boldsymbol{I}$?\n",
    "\n",
    "Turns out, this is always invertible, because we're adding a constant across the diagonals, which ensures that the columns are linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a regular linear regression model, so we can see how it compares to the models using regularization. We will use a polynomial model with `displacement` up to degree 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "x_train = np.vander(mpg_train[\"displacement\"], 13)\n",
    "y_train = mpg_train[[\"mpg\"]]\n",
    "\n",
    "x_validation = np.vander(mpg_validation[\"displacement\"], 13)\n",
    "y_validation = mpg_validation[[\"mpg\"]]\n",
    "\n",
    "# instantiate your model\n",
    "linear_model = ...\n",
    "\n",
    "# fit the model\n",
    "...\n",
    "# make predictions on validation set\n",
    "linear_prediction = ...\n",
    "# find mean squared error\n",
    "linear_loss = ...\n",
    "\n",
    "print(\"Root Mean Squared Error of linear model: {:.2f}\".format(linear_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using what you did above as reference, do the same using a Ridge regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "...\n",
    "ridge_loss = ... # mean squared error of ridge model\n",
    "\n",
    "print(\"Root Mean Squared Error of linear model: {:.2f}\".format(linear_loss))\n",
    "print(\"Root Mean Squared Error of ridge model: {:.2f}\".format(ridge_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lasso'></a>\n",
    "### LASSO\n",
    "In **LASSO** regression, the regularization function is the sum of absolute values of the weights. If there's one thing you should know about LASSO is that it is *sparsity inducing*. This just means that it forces some weights to take on zero values, leaving you with fewer explanatory variables in the resulting model than you put in. Unlike ridge regression, LASSO doesn't necessarily have a unique solution, and there's no formula that determines what the optimal weights should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "...\n",
    "lasso_loss = ... # mean squared error of lasso model\n",
    "\n",
    "print(\"Root Mean Squared Error of linear model: {:.2f}\".format(linear_loss))\n",
    "print(\"Root Mean Squared Error of lasso model: {:.2f}\".format(lasso_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Ridge and LASSO\n",
    "We just told you a lot of things about ridge and lasso, but here are some visualizations to help you understand the intuition behind some of the characteristics of these two regularization methods. Another way to describe the modified minimization function above is that it's the same loss function as before, with the *additional constraint* that $R(\\boldsymbol{\\theta}) \\leq t$. Now, $t$ is related to $\\lambda$ but the exact relationship between the two parameters depends on your data. Regardless, let's take a look at what this means in the two-dimensional case. For ridge,\n",
    "\n",
    "$$\\theta_0^2 + \\theta_1^2 \\leq t$$\n",
    "\n",
    "Does this look familiar to you? What if it's in the form $x^2 + y^2 \\leq t$? Or how about now:\n",
    "<img src='http://vikingsseason5i.com/wp-content/uploads/2018/08/circle-equation-circle-equation-unit-circle.jpg' width=400 />\n",
    "\n",
    "Lasso is of the form $$\\left|\\theta_0\\right| + \\left|\\theta_1\\right| \\leq t$$ This one's a little harder to interpret, perhaps this will help inspire you:\n",
    "<img src='https://cdn.kastatic.org/ka-perseus-graphie/3b9b8f4b4dac19e1197e9dd94553d0822f9fe69a.png' />\n",
    "\n",
    "#### Norm Balls\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/f/f8/L1_and_L2_balls.svg' width=400/>\n",
    "<img src='norm_balls.png' width=400/>\n",
    "\n",
    "The rhombus and circle as a visualization of the regularization term, while the blue circles are the topological curves representing the loss function based on the weights. You want to minimize the sum of these, which means you want to minimize each of those. The point that minimizes the sum is the minimum point at which they intersect.\n",
    "\n",
    "\n",
    "**Question**: Based on these visualizations, could you explain why LASSO is sparsity-inducing?\n",
    "\n",
    "Turns out that the $L2-norm$ is always some sort of smooth surface, from a circle in 2D to a sphere in 3D. On the other hand, LASSO always has sharp corners. This is exactly the feature that makes it sparsiy-inducing. As you might imagine, just as humans are more likely to bump into sharp corners than smooth surfaces, the loss term is also most likely to intersect the $L2-norm$ at one of the corners.\n",
    "\n",
    "### Regularization and Bias Variance\n",
    "As we mentioned earlier, the bias is the average OLS loss term across multiple models of the same family (e.g. same degree polynomial) trained on separate datasets. Variance is the average variance of the weight vectors (coefficients) on your features. \n",
    "\n",
    "Without the regularization term, we’re just minimizing bias; the regularization term means we won’t get the lowest possible bias, but we’re exchanging that for some lower variance so that our model does better at generalizing to data points outside of our training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We said that $\\lambda$ is how much we care about the regularization term, but what does that look like? Let's return to the polynomial example from last week, and see what the resulting models look like with different values of $\\lambda$ given a degree 8 polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ridgeRegularizationDemo([0, 0.5, 1.0, 5.0, 10.0], 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know what to use for $\\lambda$ (or `alpha` in the `sklearn.linear_model` constructors)?\n",
    "\n",
    "That's right, it's validation! \n",
    "\n",
    "### Validation on Lambda\n",
    "Let's try to find the best $\\lambda$ for the degree 20 polynomial on `displacement` from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.arange(0, 200) # create a list of potential lambda values\n",
    "\n",
    "# create a list containing the corresponding mean_squared_error for each lambda usinb both ridge and lasso regression\n",
    "ridge_errors = [] \n",
    "lasso_errors = []\n",
    "\n",
    "...\n",
    "\n",
    "# finds the index of the minimum value in each list\n",
    "ridge_errors.index(min(ridge_errors)), lasso_errors.index(min(lasso_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check\n",
    "\n",
    "1. What happens as $\\lambda$ increases?\n",
    "    1. bias increases, variance increases\n",
    "    2. bias increases, variance decreases\n",
    "    3. bias decreases, variance increases\n",
    "    4. bias decreases, variance decreases\n",
    "2. **True** or **False**? Bias is how much error your model makes.\n",
    "3. What is **sparsity**?\n",
    "4. For each of the following, choose **ridge**, **lasso**, **both**, or **neither**:\n",
    "    1. L1-norm\n",
    "    2. L2-norm\n",
    "    3. Induces sparsity\n",
    "    4. Has analytic (mathematical) solution\n",
    "    5. Increases bias\n",
    "    6. Increases variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (susa)",
   "language": "python",
   "name": "susa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
